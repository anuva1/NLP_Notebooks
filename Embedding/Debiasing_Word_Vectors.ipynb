{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a4c0bbf80754434b9a6371e2711bef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2233f0a9f0a44889b2cf208c46af500",
              "IPY_MODEL_23b3819899294acf9ac3dd7703637a5d",
              "IPY_MODEL_5be7c131ea4646269fc7c79175816d4f"
            ],
            "layout": "IPY_MODEL_2aeef898a4d347049529aea68abc3f57"
          }
        },
        "a2233f0a9f0a44889b2cf208c46af500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_236200840a3745e5bdda651ef95ab833",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bd12c0e45e274633a047566355fe0974",
            "value": ""
          }
        },
        "23b3819899294acf9ac3dd7703637a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d1e8d2289d2410eac5450d75ee7e096",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50c37922b3c34d5fb6af91ebd181ae59",
            "value": 1
          }
        },
        "5be7c131ea4646269fc7c79175816d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db328ce675fb475384f06676cb3674b5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fcb53f2c493b4a70bae96adb53bedcdf",
            "value": "‚Äá400000/?‚Äá[00:13&lt;00:00,‚Äá37740.11it/s]"
          }
        },
        "2aeef898a4d347049529aea68abc3f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "236200840a3745e5bdda651ef95ab833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd12c0e45e274633a047566355fe0974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d1e8d2289d2410eac5450d75ee7e096": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "50c37922b3c34d5fb6af91ebd181ae59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db328ce675fb475384f06676cb3674b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcb53f2c493b4a70bae96adb53bedcdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stjn7RhBVbCq",
        "outputId": "9189c731-11e5-4367-baef-71fe1ad8a2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "# Download required NLTK resources\n",
        "nltk.download(\"stopwords\")  # Stopwords for text processing\n",
        "nltk.download('averaged_perceptron_tagger')  # POS tagger\n",
        "nltk.download('punkt')  # Tokenizer\n",
        "\n",
        "# Import tqdm for progress tracking\n",
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to prepare an embedding map from GloVe vectors\n",
        "def prepare_embedding_map(embedding_name):\n",
        "    embedding_index = {}\n",
        "\n",
        "    # Open the corresponding GloVe file based on the embedding name\n",
        "    if embedding_name == 'glove50':\n",
        "        f = open(\"/content/glove_models/glove.6B.50d.txt\")\n",
        "    elif embedding_name == 'glove100':\n",
        "        f = open(\"/content/glove_models/glove.6B.100d.txt\")\n",
        "\n",
        "    # Read the embeddings and store them in a dictionary\n",
        "    for line in tqdm(f):\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coef = np.array(values[1:], dtype=\"float\")\n",
        "        embedding_index[word] = coef\n",
        "\n",
        "    f.close()\n",
        "    return embedding_index\n"
      ],
      "metadata": {
        "id": "nRBvMjVPVmFr"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load word embeddings and filter nouns\n",
        "word_to_vec_map = prepare_embedding_map(\"glove100\")\n",
        "print(len(word_to_vec_map))\n",
        "\n",
        "try:\n",
        "    # Attempt to get words from the embedding map's vocabulary\n",
        "    words = word_to_vec_map.vocab.keys()\n",
        "    stopword = set(stopwords.words(\"english\"))\n",
        "\n",
        "    # Filter out stopwords\n",
        "    words = [w for w in words if w.lower() not in stopword]\n",
        "    tokens = nltk.word_tokenize(\" \".join(words))\n",
        "    pos_tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Extract nouns from the tokens\n",
        "    nouns = [w for w, pos in pos_tagged if pos.startswith(\"N\")]\n",
        "    print(len(words), len(nouns))\n",
        "\n",
        "    print(len(list(word_to_vec_map.vocab.keys())), 'Vectors Found')\n",
        "\n",
        "except:\n",
        "    # Fallback in case of an AttributeError (if the vocab attribute does not exist)\n",
        "    words = word_to_vec_map.keys()\n",
        "    stopword = set(stopwords.words(\"english\"))\n",
        "\n",
        "    # Filter out stopwords\n",
        "    words = [w for w in words if w.lower() not in stopword]\n",
        "    tokens = nltk.word_tokenize(\" \".join(words))\n",
        "    pos_tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Extract nouns from the tokens\n",
        "    nouns = [w for w, pos in pos_tagged if pos.startswith(\"N\")]\n",
        "    print(len(words), len(nouns))\n",
        "    print(len(list(word_to_vec_map.keys())), 'Vectors Found')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "1a4c0bbf80754434b9a6371e2711bef5",
            "a2233f0a9f0a44889b2cf208c46af500",
            "23b3819899294acf9ac3dd7703637a5d",
            "5be7c131ea4646269fc7c79175816d4f",
            "2aeef898a4d347049529aea68abc3f57",
            "236200840a3745e5bdda651ef95ab833",
            "bd12c0e45e274633a047566355fe0974",
            "8d1e8d2289d2410eac5450d75ee7e096",
            "50c37922b3c34d5fb6af91ebd181ae59",
            "db328ce675fb475384f06676cb3674b5",
            "fcb53f2c493b4a70bae96adb53bedcdf"
          ]
        },
        "id": "SI3WU95lVmOB",
        "outputId": "b8553133-d3bb-4ab3-a59a-396baecbaa3e"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a4c0bbf80754434b9a6371e2711bef5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000\n",
            "399851 210076\n",
            "400000 Vectors Found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words),len(nouns),nouns[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7mHRXn1wTNa",
        "outputId": "36c6cf55-f20e-4c75-e56a-df81af5ca0c2"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(399851, 210076, 'year')"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Cosine Similarity Calculation\n",
        "# Cosine Similarity: CosineSim(u, v) = (u.v) / (||u|| ||v||) = cos(Œ∏)\n",
        "\n",
        "def cosine_similarity(u, v):\n",
        "    # Calculate the dot product of u and v\n",
        "    dot_prod = np.dot(u, v)\n",
        "\n",
        "    # Calculate the norms (lengths) of u and v\n",
        "    norm_u = np.sqrt(np.sum(u**2))\n",
        "    norm_v = np.sqrt(np.sum(v**2))\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    distance = dot_prod / (norm_u * norm_v)\n",
        "\n",
        "    return distance\n"
      ],
      "metadata": {
        "id": "3IuPg20MYc63"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cosine Similarity Analysis for Word Pairs\n",
        "\n",
        "# Retrieve word vectors from the embedding map\n",
        "father = word_to_vec_map[\"father\"]\n",
        "mother = word_to_vec_map[\"mother\"]\n",
        "\n",
        "basket = word_to_vec_map[\"basket\"]\n",
        "dog = word_to_vec_map[\"dog\"]\n",
        "\n",
        "france = word_to_vec_map[\"france\"]\n",
        "italy = word_to_vec_map[\"italy\"]\n",
        "paris = word_to_vec_map[\"paris\"]\n",
        "rome = word_to_vec_map[\"rome\"]\n",
        "\n",
        "# Calculate and print cosine similarities\n",
        "print(f\"Cosine similarity (father, mother): {cosine_similarity(father, mother)}\")\n",
        "print(f\"Cosine similarity (basket, dog): {cosine_similarity(basket, dog)}\")\n",
        "print(f\"Cosine similarity (france-paris, italy-rome): {cosine_similarity(france - paris, italy - rome)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOsISVYYZ_8a",
        "outputId": "0f278ea8-a783-4358-b1e5-b188c9674f31"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity (father, mother): 0.8656661174315731\n",
            "Cosine similarity (basket, dog): 0.23200237592671563\n",
            "Cosine similarity (france-paris, italy-rome): 0.7056238800453161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Word Analogy: Completing the analogy a is to b as c is to __\n",
        "\n",
        "def complete_analogy(word_a, word_b, word_c, words, word_to_vec_map):\n",
        "\n",
        "    word_a = word_a.lower()\n",
        "    word_b = word_b.lower()\n",
        "    word_c = word_c.lower()\n",
        "\n",
        "    e_a = word_to_vec_map[word_a]\n",
        "    e_b = word_to_vec_map[word_b]\n",
        "    e_c = word_to_vec_map[word_c]\n",
        "\n",
        "    max_cosine_similarity = -100\n",
        "    best_word = None\n",
        "\n",
        "    # Iterate through the list of words to find the best match\n",
        "    for w in words:\n",
        "        if w in [word_a, word_b, word_c]:\n",
        "            continue  # Skip if the word is part of the analogy\n",
        "\n",
        "        if w in word_to_vec_map.keys():\n",
        "            # Calculate cosine similarity for the analogy\n",
        "            cosine_sim = cosine_similarity((e_b - e_a), (word_to_vec_map[w] - e_c))\n",
        "            if cosine_sim > max_cosine_similarity:\n",
        "                max_cosine_similarity = cosine_sim\n",
        "                best_word = w  # Update the best matching word\n",
        "\n",
        "    return best_word  # Return the best word that completes the analogy\n"
      ],
      "metadata": {
        "id": "hrxs-xYtaO2h"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Testing Word Analogy Function with Examples\n",
        "\n",
        "examples = [\n",
        "    ('italy', 'italian', 'spain'),\n",
        "    ('india', 'delhi', 'japan'),\n",
        "    ('man', 'woman', 'boy'),\n",
        "    ('small', 'smaller', 'large')\n",
        "]\n",
        "\n",
        "# Iterate through each example to print the analogy results\n",
        "for ele in examples:\n",
        "    print(f\"Analogy of {ele[0]} : {ele[1]} :: {ele[2]} : {complete_analogy(ele[0], ele[1], ele[2], words, word_to_vec_map)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm4xSHvIcD80",
        "outputId": "a605d2ec-9b9f-42ed-bc4b-80609e1b7e9d"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analogy of italy : italian :: spain : spanish\n",
            "Analogy of india : delhi :: japan : osaka\n",
            "Analogy of man : woman :: boy : girl\n",
            "Analogy of small : smaller :: large : larger\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Debiasing Word Vectors\n",
        "\n",
        "# Calculate the gender direction vector\n",
        "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
        "\n",
        "# Get the length of the gender direction vector\n",
        "length_g = len(g)\n",
        "print(f\"Length of the gender direction vector: {length_g}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYcmYFxB4fVs",
        "outputId": "ff6d9ee4-2396-424a-a994-6945e94a36ef"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the gender direction vector: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Similarity of Names with Gender Vector (Man-Woman)\n",
        "\n",
        "# List of names to compare with the gender direction vector\n",
        "names_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza']\n",
        "\n",
        "print(\"List of names and their similarity with the vector (man-woman):\")\n",
        "\n",
        "# Calculate and print cosine similarity of each name vector with the gender vector g\n",
        "for name in names_list:\n",
        "    similarity = cosine_similarity(word_to_vec_map[name], g)\n",
        "    print(f\"{name}: {similarity}\")\n",
        "\n",
        "# Note: Female names are expected to show positive correlation with the constructed vector g.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gUYicgx58mg",
        "outputId": "3aca8049-239e-441d-8727-24e5bc0ada32"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of names and their similarity with the vector (man-woman):\n",
            "john: -0.22835017436721872\n",
            "marie: 0.24537338638963357\n",
            "sophie: 0.20268358510223194\n",
            "ronaldo: -0.3328964498414265\n",
            "priya: 0.13922857114427084\n",
            "rahul: -0.0639072688424743\n",
            "danielle: 0.14913149265020167\n",
            "reza: -0.08192654617678002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Similarity of Other Words with Gender Vector (Man-Woman)\n",
        "\n",
        "# List of words to compare with the gender direction vector\n",
        "word_list = [\n",
        "    'lipstick', 'guns', 'science', 'arts', 'literature',\n",
        "    'warrior', 'doctor', 'tree', 'receptionist',\n",
        "    'technology', 'fashion', 'teacher', 'engineer',\n",
        "    'pilot', 'computer', 'singer'\n",
        "]\n",
        "\n",
        "print(\"Other words and their similarity with the vector (man-woman):\")\n",
        "\n",
        "# Calculate and print cosine similarity of each word vector with the gender vector g\n",
        "for word in word_list:\n",
        "    similarity = cosine_similarity(word_to_vec_map[word], g)\n",
        "    print(f\"{word}: {similarity}\")\n",
        "\n",
        "# This analysis highlights how various words relate to the gender bias represented by the vector g.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t49gd9I6XOM",
        "outputId": "09b5bedd-7071-401b-ac5d-0958b7f4fd1a"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Other words and their similarity with the vector (man-woman):\n",
            "lipstick: 0.18037245461893886\n",
            "guns: -0.09964446323350887\n",
            "science: -0.02147576571900459\n",
            "arts: 0.01484674744156461\n",
            "literature: 0.08261854474431136\n",
            "warrior: -0.15634200481756028\n",
            "doctor: 0.10942282324077059\n",
            "tree: -0.08868359642037957\n",
            "receptionist: 0.2806875926160281\n",
            "technology: -0.14474526940138327\n",
            "fashion: 0.08097436821066459\n",
            "teacher: 0.1523369596791014\n",
            "engineer: -0.12300012058033458\n",
            "pilot: -0.04113394172314754\n",
            "computer: -0.11545715478097537\n",
            "singer: 0.11372642801434334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Highest Similarity: The word \"receptionist\" has the highest positive similarity score (0.2807),\n",
        "indicating a stronger association with the vector (man-woman).\n",
        "\n",
        "Lowest Similarity: The word \"warrior\" has the most negative similarity (-0.1563),\n",
        "suggesting it is least associated with the concept represented by (man-woman).\n",
        "\n",
        "Overall Trends: Words related to traditionally feminine roles (like \"lipstick\" and \"receptionist\")\n",
        "tend to have positive similarity values, while terms associated\n",
        "with masculinity or neutrality (like \"guns\" and \"warrior\") tend to show negative values.\n",
        "\n",
        "This is a biased trend as expected.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8McSKOPX6w3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Neutralize bias for non-gender specific words\n",
        "\n",
        "\"\"\"\n",
        "This formula helps in debiasing word embeddings by removing the component of a word vector that lies in the bias direction.\n",
        "Here‚Äôs a step-by-step explanation:\n",
        "\n",
        "Word Embeddings: Words are represented as vectors in an n-dimensional space.\n",
        "For example, the word \"receptionist\" is represented by a vector e.\n",
        "In this vector space, certain directions can encode bias, such as gender bias.\n",
        "\n",
        "Bias Direction\n",
        "g: The vector g represents the \"bias direction\" in the embedding space,\n",
        " which might, for example, encode male/female associations.\n",
        "\n",
        "Neutralization: The goal is to \"neutralize\" a word like \"receptionist\" by removing any gender-related information,\n",
        "i.e., the component of the word vector that lies in the direction of bias g.\n",
        "\n",
        "Step 1 ‚Äì Calculating the Bias Component:\n",
        "\n",
        "The formula\n",
        "bias_component = (ùëí‚ãÖùëî / ‚à£‚à£ùëî‚à£‚à£2)‚àóùëî computes the projection of the word vector e onto the bias direction g.\n",
        "This projection is the portion of the word vector that lies along the bias direction.\n",
        "It tells us how much of the vector e is aligned with g.\n",
        "\n",
        "Step 2 ‚Äì Debiasing:\n",
        "\n",
        "The next step is to subtract this bias component from the original word vector:\n",
        "debias = ùëí ‚àí e bias_component\n",
        "\n",
        "This subtraction \"neutralizes\" the bias by removing the part of the word vector that lies in the bias direction g,\n",
        "leaving only the part of the vector that is orthogonal (perpendicular) to g.\n",
        "\n",
        "By performing this operation, the vector for \"receptionist\" no longer contains gender information\n",
        "but retains all other meaningful information.\n",
        "This ensures that words like \"receptionist\" are not biased by gender in downstream tasks.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "74ISahl_7fV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Neutralize Gender Bias in Word Vectors\n",
        "\n",
        "def neutralize(word, g, word_to_vec_map):\n",
        "    # Calculate the dot product of the word vector and the gender vector\n",
        "    dot_p = np.dot(word_to_vec_map[word], g)\n",
        "\n",
        "    # Determine the bias component to be removed\n",
        "    e_bias = (dot_p / np.sum(g ** 2)) * g\n",
        "\n",
        "    # Remove the bias from the original word vector\n",
        "    e_unbiased = word_to_vec_map[word] - e_bias\n",
        "    return e_unbiased\n"
      ],
      "metadata": {
        "id": "C3SNz_Cz7fcs"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the word to analyze\n",
        "e = \"receptionist\"\n",
        "\n",
        "# Calculate and print cosine similarity before neutralizing bias\n",
        "similarity_before = cosine_similarity(word_to_vec_map[e], g)\n",
        "print(f\"Cosine similarity between '{e}' and g before neutralizing: {similarity_before}\")\n",
        "\n",
        "# Neutralize the bias and calculate the similarity again\n",
        "e_unbiased = neutralize(e, g, word_to_vec_map)\n",
        "similarity_after = cosine_similarity(e_unbiased, g)\n",
        "print(f\"Cosine similarity between '{e}' and g after neutralizing: {similarity_after}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJKj-oOUH9yp",
        "outputId": "a906815c-adf1-4256-e7b4-873673494bce"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'receptionist' and g before neutralizing: 0.2806875926160281\n",
            "Cosine similarity between 'receptionist' and g after neutralizing: -2.9382195433746986e-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Equalization algorithm for gender-specific words\n",
        "\"\"\"\n",
        "Equalization is applied to pairs of words that you might want to have differ only through the gender property.\n",
        "As a concrete example, suppose that \"actress\" is closer to \"babysit\" than \"actor.\" By applying neutralizing to \"babysit\" we can reduce the gender-stereotype\n",
        "associated with babysitting. But this still does not guarantee that \"actor\" and \"actress\" are equidistant from \"babysit.\"\n",
        "The equalization algorithm takes care of this.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "M2sxSjHcImz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def equalize(pair, bias_axis, word_to_vec_map):\n",
        "    w1, w2 = pair\n",
        "    e_w1 = word_to_vec_map[w1]\n",
        "    e_w2 = word_to_vec_map[w2]\n",
        "\n",
        "    mu = (e_w1 + e_w2) / 2\n",
        "    mu_b = (np.dot(mu, bias_axis) / np.sum(bias_axis**2)) * bias_axis  # Normalized\n",
        "    mu_orth = mu - mu_b\n",
        "\n",
        "    e_w1B = (np.dot(e_w1, bias_axis) / np.sum(bias_axis**2)) * bias_axis  # Normalized\n",
        "    e_w2B = (np.dot(e_w2, bias_axis) / np.sum(bias_axis**2)) * bias_axis  # Normalized\n",
        "\n",
        "    # Avoid division by zero\n",
        "    norm_w1 = np.linalg.norm(e_w1B - mu_orth)\n",
        "    norm_w2 = np.linalg.norm(e_w2B - mu_orth)\n",
        "\n",
        "    if norm_w1 == 0:\n",
        "        e_w1B_unbiased = np.zeros_like(e_w1B)\n",
        "    else:\n",
        "        e_w1B_unbiased = np.sqrt(np.abs(1 - mu_orth**2)) * (e_w1B - mu_b) / norm_w1\n",
        "\n",
        "    if norm_w2 == 0:\n",
        "        e_w2B_unbiased = np.zeros_like(e_w2B)\n",
        "    else:\n",
        "        e_w2B_unbiased = np.sqrt(np.abs(1 - mu_orth**2)) * (e_w2B - mu_b) / norm_w2\n",
        "\n",
        "    e_w1B_corrected = e_w1B_unbiased + mu_orth\n",
        "    e_w2B_corrected = e_w2B_unbiased + mu_orth\n",
        "\n",
        "    return e_w1B_corrected, e_w2B_corrected\n"
      ],
      "metadata": {
        "id": "KubvQMHII7mO"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display cosine similarities before equalizing\n",
        "print(\"Cosine similarities before equalizing:\")\n",
        "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) =\", cosine_similarity(word_to_vec_map[\"man\"], g))\n",
        "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) =\", cosine_similarity(word_to_vec_map[\"woman\"], g))\n",
        "print()\n",
        "\n",
        "# Equalize the embeddings for 'man' and 'woman'\n",
        "e1, e2 = equalize((\"man\", \"woman\"), g, word_to_vec_map)\n",
        "\n",
        "# Display cosine similarities after equalizing\n",
        "print(\"Cosine similarities after equalizing:\")\n",
        "print(f\"cosine_similarity(e1, gender) = {cosine_similarity(e1, g)}\")\n",
        "print(f\"cosine_similarity(e2, gender) = {cosine_similarity(e2, g)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bkl4vDOI7pZ",
        "outputId": "427b0a9c-c866-494a-89cb-3fd463624856"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarities before equalizing:\n",
            "cosine_similarity(word_to_vec_map[\"man\"], gender) = -0.18769064329512627\n",
            "cosine_similarity(word_to_vec_map[\"woman\"], gender) = 0.388177000149502\n",
            "\n",
            "Cosine similarities after equalizing:\n",
            "cosine_similarity(e1, gender) = -0.045804611352558575\n",
            "cosine_similarity(e2, gender) = 0.04331400245481595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################### END OF NOTEBOOK #########################################################################"
      ],
      "metadata": {
        "id": "eYoK9zhtI7ul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}