{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import word2vec,FastText"
      ],
      "metadata": {
        "id": "KRhVK7Nhsou5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dxTc0kihhkFm"
      },
      "outputs": [],
      "source": [
        "### One Hot Encoding (OHE) Of Word Vectors\n",
        "# In this method, each word in the vocabulary V is assigned an integer index, i(from 0 to V-1) & the vector representation for each word\n",
        "# is of the length V with all 0s except 1 at the ith index for the corresponding word. It is a word-level representation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"the dictionary has many examples of how words are used\"\n",
        "\n",
        "def OHE(text):\n",
        "  tokens = set(text.lower().split())\n",
        "  length = len(tokens)\n",
        "  index_map = { val:index for val,index in zip(tokens,range(length))}\n",
        "  ohe_matrix = {}\n",
        "\n",
        "\n",
        "  for token in tokens :\n",
        "    ohe = np.zeros(length)\n",
        "    ohe[index_map[token]] = 1\n",
        "    ohe_matrix[token] = ohe\n",
        "\n",
        "  return ohe_matrix\n",
        "\n",
        "\n",
        "ohe_result = OHE(sample_sentence)\n",
        "ohe_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odKhmFwLskz3",
        "outputId": "ebc3260f-49d0-4a1f-e57a-a0fa4fbe7267"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'examples': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'used': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'words': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
              " 'dictionary': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
              " 'are': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
              " 'of': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
              " 'many': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
              " 'how': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
              " 'has': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The vector representation doesn’t hold any semantic meaning,\n",
        "example: If we get boy, boys & jail as 3 tokens, we know vector representation for boy & boys should be closer\n",
        "but in the case of OHE, every token is at the same distance from any other token.\n",
        "\n",
        "Sparse representation (most values 0 in presentation) can lead to computational inefficiencies.\n",
        "\n",
        "OOV (out of vocabulary) problem.\n",
        "\n",
        "Very high dimension as each word has a vector length of total vocab, the dimensionality curse.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "ZdAJ8luItM9g",
        "outputId": "ca5253e2-938a-408b-e4e3-12b8b0c420b9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe vector representation doesn’t hold any semantic meaning,\\nexample: If we get boy, boys & jail as 3 tokens, we know vector representation for boy & boys should be closer \\nbut in the case of OHE, every token is at the same distance from any other token. \\n\\nSparse representation (most values 0 in presentation) can lead to computational inefficiencies.\\n\\nOOV (out of vocabulary) problem.\\n\\nVery high dimension as each word has a vector length of total vocab, the dimensionality curse.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Bag of Words (BoW)\n",
        "# Bag Of Word is indeed just a bag of words ignoring 2 crucial things in its vector representations 1) Order of tokens in which they appear in sentence/sequence and 2) Semantic meaning of the token.\n",
        "# Bag of Words is majorly based on the frequency of tokens present in a sentence & nothing else.\n",
        "# It is a sentence-level representation."
      ],
      "metadata": {
        "id": "PpKnUJlfRNZt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentences = [\"the dictionary has many examples of how words are used\",\n",
        "                    \"The more you practice, the more confident you become\"]\n",
        "\n",
        "cv = CountVectorizer()\n",
        "bow_rep = cv.fit_transform(sample_sentences)\n",
        "\n",
        "print(cv.vocabulary_)\n",
        "print(f\"BOW Representation for {sample_sentences[0]} is {bow_rep[0].toarray()}\")\n",
        "print(f\"BOW Representation for {sample_sentences[1]} is {bow_rep[1].toarray()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTW0JMoUSKOf",
        "outputId": "6ba84341-e000-4d3e-e6ad-05fe2712d0e5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 11, 'dictionary': 3, 'has': 5, 'many': 7, 'examples': 4, 'of': 9, 'how': 6, 'words': 13, 'are': 0, 'used': 12, 'more': 8, 'you': 14, 'practice': 10, 'confident': 2, 'become': 1}\n",
            "BOW Representation for the dictionary has many examples of how words are used is [[1 0 0 1 1 1 1 1 0 1 0 1 1 1 0]]\n",
            "BOW Representation for The more you practice, the more confident you become is [[0 1 1 0 0 0 0 0 2 0 1 2 0 0 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The representation is now of fixed length irrespective of the sentence length\n",
        "The representation dimension has reduced drastically compared to OHE where we would have such vector representation for just one token/word.\n",
        "Though, as vocab can be huge, the representation can still be parsed.\n",
        "\n",
        "Any sentence with the same words will have a similar presentation. Though, slight variations can make the representation drastically different.\n",
        "‘I run’ & ‘they ran’ will have completely different representation\n",
        "\n",
        "OOV (out of vocabulary) problem.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "MbwCnCaSSKRi",
        "outputId": "75b2f517-00bc-48f2-9636-597c5ea91a66"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nThe representation is now of fixed length irrespective of the sentence length\\nThe representation dimension has reduced drastically compared to OHE where we would have such vector representation for just one token/word. \\nThough, as vocab can be huge, the representation can still be parsed.\\n\\nAny sentence with the same words will have a similar presentation. Though, slight variations can make the representation drastically different. \\n‘I run’ & ‘they ran’ will have completely different representation\\n\\nOOV (out of vocabulary) problem.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Bag of N-grams (BoN)\n",
        "# Before this the major drawback observed was the method was missing out on the context of the words used.\n",
        "# By grouping continuous words together, we may be able to capture some meaning.\n",
        "# The BoW can be considered as BoN with n=1."
      ],
      "metadata": {
        "id": "mXy0MePlUwnL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(ngram_range=(3,3))\n",
        "sample_sentences = [\"the dictionary has many examples of how words are used\",\n",
        "                    \"The more you practice, the more confident you become\"]\n",
        "\n",
        "ngram_rep = cv.fit_transform(sample_sentences)\n",
        "print(cv.vocabulary_)\n",
        "\n",
        "print(f\"N-GRAM Representation for {sample_sentences[0]} is {ngram_rep[0].toarray()}\")\n",
        "print(f\"N-GRAM Representation for {sample_sentences[1]} is {ngram_rep[1].toarray()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58OOjKkSVH2Y",
        "outputId": "543b3c38-8ec6-47dd-ef5e-ab0df75a0e3c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the dictionary has': 10, 'dictionary has many': 1, 'has many examples': 3, 'many examples of': 5, 'examples of how': 2, 'of how words': 8, 'how words are': 4, 'words are used': 13, 'the more you': 12, 'more you practice': 7, 'you practice the': 14, 'practice the more': 9, 'the more confident': 11, 'more confident you': 6, 'confident you become': 0}\n",
            "N-GRAM Representation for the dictionary has many examples of how words are used is [[0 1 1 1 1 1 0 0 1 0 1 0 0 1 0]]\n",
            "N-GRAM Representation for The more you practice, the more confident you become is [[1 0 0 0 0 0 1 1 0 1 0 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "N-GRAM representation does improve semantic understanding of the sentence & sentences with the same phrase/group of words will have a similar representation.\n",
        "\n",
        "Though we still have a long way to go as :\n",
        "\n",
        "We haven’t found a solution to OOVs\n",
        "Sparse representation poses a big challenge\n",
        "As vocab increases, the dimensionality of representation increases\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "odKPUeuNV3-V",
        "outputId": "41b89902-e708-4cee-99b3-869bf823ae4f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nN-GRAM representation does improve semantic understanding of the sentence & sentences with the same phrase/group of words will have a similar representation. \\n\\nThough we still have a long way to go as : \\n\\nWe haven’t found a solution to OOVs\\nSparse representation poses a big challenge\\nAs vocab increases, the dimensionality of representation increases\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "CPJ9i2EqWid3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf = TfidfVectorizer()\n",
        "sample_sentences = [\"the dictionary has many examples of how words are used\",\n",
        "                    \"the more you practice, the more confident you become\"]\n",
        "\n",
        "tf_rep = tf.fit_transform(sample_sentences)\n",
        "print(tf.vocabulary_)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"TfIdf Representation for {sample_sentences[0]} is {tf_rep[0].toarray()} \\n\")\n",
        "print(f\"TfIdf Representation for {sample_sentences[1]} is {tf_rep[1].toarray()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f43jOdZmllzq",
        "outputId": "8aaf379a-6674-4c5a-978f-ea3014d8adef"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 11, 'dictionary': 3, 'has': 5, 'many': 7, 'examples': 4, 'of': 9, 'how': 6, 'words': 13, 'are': 0, 'used': 12, 'more': 8, 'you': 14, 'practice': 10, 'confident': 2, 'become': 1}\n",
            "\n",
            "\n",
            "TfIdf Representation for the dictionary has many examples of how words are used is [[0.32433627 0.         0.         0.32433627 0.32433627 0.32433627\n",
            "  0.32433627 0.32433627 0.         0.32433627 0.         0.23076793\n",
            "  0.32433627 0.32433627 0.        ]] \n",
            "\n",
            "TfIdf Representation for the more you practice, the more confident you become is [[0.         0.27708406 0.27708406 0.         0.         0.\n",
            "  0.         0.         0.55416811 0.         0.27708406 0.39429518\n",
            "  0.         0.         0.55416811]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TF-IDF is a single float value per word that solves a very particular problem that may come in handy in text classification a lot i.e. word importance\n",
        "i.e. how important a particular word is in a document/sentence.\n",
        "It is a sentence-level representation\n",
        "\n",
        "It moves with intuition than if a certain word is present in every other sentence, chances that it might not be important.\n",
        "For example: is, a, the, etc. while words that are rarely found in the sentences may be of higher importance.\n",
        "\n",
        "TF(x) : (Frequency of word ‘x’ in sentence s) / (total tokens in sentence s)\n",
        "IDF(x): log(total sentences/total sentences with word ‘x’)\n",
        "TF_IDF(x): TF(x) * IDF(x)\n",
        "\n",
        "F-IDF also struggles with the same problems of handling OOVs, sparsity & big dimensions but it does capture some semantics as well.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "nTphSrcr2uhJ",
        "outputId": "2a8b619f-1393-48cc-e688-195baac74854"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTF-IDF is a single float value per word that solves a very particular problem that may come in handy in text classification a lot i.e. word importance\\ni.e. how important a particular word is in a document/sentence. \\nIt is a sentence-level representation\\n\\nIt moves with intuition than if a certain word is present in every other sentence, chances that it might not be important. \\nFor example: is, a, the, etc. while words that are rarely found in the sentences may be of higher importance.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "uVGT7VFf4UlH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_para = \"\"\"Bali is predominantly a Hindu country. Bali is known for its elaborate, traditional dancing. The dancing is inspired by its Hindi beliefs. Most of the dancing portrays tales of good versus evil. To watch the dancing is a breathtaking experience. Lombok has some impressive points of interest – the majestic Gunung Rinjani is an active volcano. It is the second highest peak in Indonesia. Art is a Balinese passion. Batik paintings and carved statues make popular souvenirs. Artists can be seen whittling and painting on the streets, particularly in Ubud. It is easy to appreciate each island as an attractive tourist destination. Majestic scenery; rich culture; white sands and warm, azure waters draw visitors like magnets every year. Snorkelling and diving around the nearby Gili Islands is magnificent. Marine fish, starfish, turtles and coral reef are present in abundance. Bali and Lombok are part of the Indonesian archipelago. Bali has some spectacular temples. The most significant is the Mother Temple, Besakih. The inhabitants of Lombok are mostly Muslim with a Hindu minority. Lombok remains the most understated of the two islands. Lombok has several temples worthy of a visit, though they are less prolific. Bali and Lombok are neighbouring islands.\"\"\"\n",
        "text = [ word.lower().split() for word in sample_para.split(\".\") ]\n",
        "\n",
        "skipgram = word2vec.Word2Vec(text, window = 2, vector_size = 100, sg = 1, min_count = 1)\n",
        "cbow = word2vec.Word2Vec(text, window = 2, vector_size = 100, sg = 0, min_count = 1)\n",
        "\n",
        "print (f\"Skipgram : {skipgram.wv.most_similar('dancing', topn=5)} \")\n",
        "print (f\"cbow : {cbow.wv.most_similar('dancing', topn=5)} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvwijhhX6ina",
        "outputId": "98a58f00-5773-4e1f-b0a2-4b0997fb9da6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipgram : [('art', 0.285526841878891), ('inspired', 0.2409186065196991), ('remains', 0.21204082667827606), ('visitors', 0.20760729908943176), ('of', 0.20495805144309998)] \n",
            "cbow : [('art', 0.285190224647522), ('inspired', 0.239618718624115), ('remains', 0.21129843592643738), ('visitors', 0.2056775540113449), ('of', 0.20177268981933594)] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Continuous Bag Of Words (CBoW)\n",
        "\n",
        "A very popular name in the word embedding space, Word2Vec is a neural network-based model for learning word embeddings.\n",
        "It does solve the 2 most critical problems: lower dimension representations & capturing the meaning of the words using the context (vicinity words).\n",
        "\n",
        "The idea behind CBoW is to train a NN that gives context words (vicinity words) as input & predicts the target word.\n",
        "Choose an even number ‘m’.\n",
        "Now for a target word, we will consider its ‘m’ neighboring word on either side (left & right) & prepare training data set.\n",
        "Here, given the source text, for each word, we prepare training samples where vicinity words act as features & the blue highlighted ones are the target\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "kcld-nHz4vf9",
        "outputId": "246f546c-bff1-41db-8f10-7444699c1d03"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nA very popular name in the word embedding space, Word2Vec is a neural network-based model for learning word embeddings. \\nIt does solve the 2 most critical problems: lower dimension representations & capturing the meaning of the words using the context (vicinity words).\\n\\nThe idea behind CBoW is to train a NN that gives context words (vicinity words) as input & predicts the target word.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Skip Gram\n",
        "\n",
        "An exact opposite to CBoW, Skip Gram tries to predict context words given a single word as input.\n",
        "Select ‘m’ similar to CBoW. We will be considering m words on both sides of every word for prediction but as separate samples.\n",
        "The blue token becomes the input.\n",
        "Next, similar to CBoW, we have a 2-layered NN where each vicinity word is fed & the same target word is predicted against each input word.\n",
        "\n",
        "Interestingly, we require the weight of the hidden layer that acts as word embedding. The model trained can be dumped !!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9dS6VZbf4vi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Global Vectors (GloVe)"
      ],
      "metadata": {
        "id": "gkeEuiYo4vl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In the case of Word2Vec, only vicinity words are used to derive an embedding for a given word.\n",
        "This may be restrictive in the long run as the embedding will have a limited context.\n",
        "GloVe, a word-level vector representation scheme, use the entire corpus to derive embedding for a word.\n",
        "\n",
        "GloVe uses word-word co-occurrence probabilities to incorporate the essence of the entire corpus.\n",
        "As it is a global statistic(derived using the entire dataset), the embedding generated is called GloVe or Global Vectors.\n",
        "\n",
        "A word-word co-occurrence matrix is nothing but a 2d array that stores frequency of every possible pair of word in the corpus.\n",
        "It can be imagined as a correlation matrix structure with instead of correlation values,\n",
        "we have frequency of the 2 words occurring together in a specified window.\n",
        "we will calculate word-word cooccurrence probabilities using the above matrix.\n",
        "For P(A|B) = Freq(A∩B)/Freq(B)\n",
        "\n",
        "P(k|ice)/P(k|steam) >>1 ,k is more associated with ice. As in case of P(solid|ice)/P(solid|steam)=8.9\n",
        "P(k|ice)/P(k|steam) <<1, k is more related to steam. As P(gas|ice)/P(gas|steam)=0.085\n",
        "P(k|ice)/P(k|steam)≈1, k is either related to both or unrelated. As in case of water & fashion.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2JILPQH989zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### fastText"
      ],
      "metadata": {
        "id": "0cA3k4qr892t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_para = \"\"\"Bali is predominantly a Hindu country. Bali is known for its elaborate, traditional dancing. The dancing is inspired by its Hindi beliefs. Most of the dancing portrays tales of good versus evil. To watch the dancing is a breathtaking experience. Lombok has some impressive points of interest – the majestic Gunung Rinjani is an active volcano. It is the second highest peak in Indonesia. Art is a Balinese passion. Batik paintings and carved statues make popular souvenirs. Artists can be seen whittling and painting on the streets, particularly in Ubud. It is easy to appreciate each island as an attractive tourist destination. Majestic scenery; rich culture; white sands and warm, azure waters draw visitors like magnets every year. Snorkelling and diving around the nearby Gili Islands is magnificent. Marine fish, starfish, turtles and coral reef are present in abundance. Bali and Lombok are part of the Indonesian archipelago. Bali has some spectacular temples. The most significant is the Mother Temple, Besakih. The inhabitants of Lombok are mostly Muslim with a Hindu minority. Lombok remains the most understated of the two islands. Lombok has several temples worthy of a visit, though they are less prolific. Bali and Lombok are neighbouring islands.\"\"\"\n",
        "text = [ word.lower().split() for word in sample_para.split(\".\") ]\n",
        "\n",
        "\n",
        "fasttext = FastText(vector_size=100, window=2, min_count=1)\n",
        "fasttext.build_vocab(text)\n",
        "fasttext.train(text, total_examples=len(text), epochs=10)\n",
        "\n",
        "print (f\"Fasttext : {fasttext.wv.most_similar('dancing', topn=5)} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDA_Q1sm896F",
        "outputId": "ec1bc377-b8c3-4f5a-b1dd-dd3556f68f73"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fasttext : [('year', 0.2896951735019684), ('painting', 0.28574812412261963), ('predominantly', 0.24354396760463715), ('by', 0.22881247103214264), ('they', 0.21968737244606018)] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################# END OF NOTEBOOK ############################################################"
      ],
      "metadata": {
        "id": "lpXpXqlh-hGy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}